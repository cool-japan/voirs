//! GPU Acceleration System
//!
//! This module provides GPU acceleration capabilities for voice cloning operations
//! using CUDA/OpenCL through the Candle framework. It includes GPU memory management,
//! tensor operations acceleration, and automatic fallback to CPU when GPU is unavailable.

use crate::{Error, Result};
use candle_core::{DType, Device, Tensor};
use candle_nn;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::{Arc, Mutex, RwLock};
use std::time::{Duration, Instant};

/// GPU device types supported
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub enum GpuDeviceType {
    /// NVIDIA CUDA device
    Cuda,
    /// CPU fallback
    Cpu,
    /// Metal (macOS GPU acceleration)
    Metal,
}

/// GPU memory statistics
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct GpuMemoryStats {
    /// Total GPU memory in bytes
    pub total_memory: u64,
    /// Used GPU memory in bytes
    pub used_memory: u64,
    /// Free GPU memory in bytes
    pub free_memory: u64,
    /// Peak memory usage during session
    pub peak_memory: u64,
    /// Number of active tensors on GPU
    pub active_tensors: usize,
    /// Memory fragmentation ratio (0.0 to 1.0)
    pub fragmentation_ratio: f32,
}

/// GPU performance metrics
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct GpuPerformanceMetrics {
    /// GPU utilization percentage (0.0 to 1.0)
    pub gpu_utilization: f32,
    /// Memory bandwidth utilization (0.0 to 1.0)
    pub memory_bandwidth: f32,
    /// Average kernel execution time
    pub avg_kernel_time: Duration,
    /// Total operations executed on GPU
    pub operations_count: u64,
    /// GPU temperature (Celsius)
    pub temperature: f32,
    /// Power consumption (Watts)
    pub power_consumption: f32,
}

/// Configuration for GPU acceleration
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct GpuAccelerationConfig {
    /// Enable GPU acceleration
    pub enabled: bool,
    /// Preferred device type
    pub device_type: GpuDeviceType,
    /// GPU device ID to use (for multi-GPU systems)
    pub device_id: usize,
    /// Memory pool size in MB (0 = auto)
    pub memory_pool_size_mb: usize,
    /// Enable mixed precision (FP16)
    pub mixed_precision: bool,
    /// Tensor core usage
    pub use_tensor_cores: bool,
    /// Enable memory optimization
    pub memory_optimization: bool,
    /// Automatic fallback to CPU if GPU fails
    pub auto_fallback: bool,
    /// Maximum GPU memory utilization (0.0 to 1.0)
    pub max_memory_usage: f32,
    /// Enable asynchronous GPU operations
    pub async_operations: bool,
    /// Batch size for GPU operations
    pub batch_size: usize,
}

impl Default for GpuAccelerationConfig {
    fn default() -> Self {
        Self {
            enabled: true,
            device_type: GpuDeviceType::Cuda,
            device_id: 0,
            memory_pool_size_mb: 0, // Auto-detect
            mixed_precision: true,
            use_tensor_cores: true,
            memory_optimization: true,
            auto_fallback: true,
            max_memory_usage: 0.8,
            async_operations: true,
            batch_size: 32,
        }
    }
}

/// GPU tensor operation types
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub enum GpuOperationType {
    /// Matrix multiplication
    MatMul,
    /// Convolution operations
    Conv,
    /// Embedding lookup
    Embedding,
    /// Attention computation
    Attention,
    /// Activation functions
    Activation,
    /// Normalization operations
    Normalization,
    /// Audio processing operations
    AudioProcessing,
    /// Speaker adaptation operations
    SpeakerAdaptation,
}

/// Tensor operation request
#[derive(Debug)]
pub struct TensorOperation {
    /// Operation type
    pub operation_type: GpuOperationType,
    /// Input tensors
    pub inputs: Vec<Tensor>,
    /// Operation parameters
    pub parameters: HashMap<String, f32>,
    /// Priority (0 = lowest, 10 = highest)
    pub priority: u8,
}

/// Result of tensor operation
#[derive(Debug)]
pub struct TensorOperationResult {
    /// Output tensors
    pub outputs: Vec<Tensor>,
    /// Execution time on GPU
    pub execution_time: Duration,
    /// Memory used for operation
    pub memory_used: u64,
    /// Operation metadata
    pub metadata: HashMap<String, f32>,
}

/// GPU memory pool for efficient tensor allocation
struct GpuMemoryPool {
    device: Device,
    allocated_blocks: HashMap<usize, usize>,
    free_blocks: HashMap<usize, usize>,
    total_allocated: u64,
    peak_usage: u64,
}

impl GpuMemoryPool {
    fn new(device: Device) -> Self {
        Self {
            device,
            allocated_blocks: HashMap::new(),
            free_blocks: HashMap::new(),
            total_allocated: 0,
            peak_usage: 0,
        }
    }

    fn allocate(&mut self, size: usize) -> Result<usize> {
        // Try to reuse existing block
        if let Some(&free_count) = self.free_blocks.get(&size) {
            if free_count > 0 {
                self.free_blocks.insert(size, free_count - 1);
                return Ok(size); // Return size as a mock pointer ID
            }
        }

        // Allocate new block (simplified implementation)
        let allocated_count = self.allocated_blocks.entry(size).or_insert(0);
        *allocated_count += 1;
        self.total_allocated += size as u64;
        self.peak_usage = self.peak_usage.max(self.total_allocated);

        Ok(size) // Return size as a mock pointer ID
    }

    fn deallocate(&mut self, _ptr_id: usize, size: usize) {
        let free_count = self.free_blocks.entry(size).or_insert(0);
        *free_count += 1;
    }

    fn get_stats(&self) -> GpuMemoryStats {
        let used_memory = self.total_allocated;
        let free_blocks_count: usize = self.free_blocks.values().sum();

        GpuMemoryStats {
            total_memory: 8_000_000_000, // 8GB example
            used_memory,
            free_memory: 8_000_000_000 - used_memory,
            peak_memory: self.peak_usage,
            active_tensors: self.allocated_blocks.len(),
            fragmentation_ratio: 0.1, // Simplified calculation
        }
    }
}

/// Main GPU acceleration system
#[derive(Clone)]
pub struct GpuAccelerator {
    config: GpuAccelerationConfig,
    device: Device,
    memory_pool: Arc<Mutex<GpuMemoryPool>>,
    performance_metrics: Arc<RwLock<GpuPerformanceMetrics>>,
    operation_queue: Arc<Mutex<Vec<TensorOperation>>>,
    tensor_cache: Arc<RwLock<HashMap<String, Tensor>>>,
    is_available: bool,
}

impl GpuAccelerator {
    /// Create new GPU accelerator
    pub fn new(config: GpuAccelerationConfig) -> Result<Self> {
        let device = Self::create_device(&config)?;
        let is_available = Self::check_gpu_availability(&device);

        let memory_pool = Arc::new(Mutex::new(GpuMemoryPool::new(device.clone())));
        let performance_metrics = Arc::new(RwLock::new(GpuPerformanceMetrics {
            gpu_utilization: 0.0,
            memory_bandwidth: 0.0,
            avg_kernel_time: Duration::from_millis(0),
            operations_count: 0,
            temperature: 25.0,
            power_consumption: 0.0,
        }));

        Ok(Self {
            config,
            device,
            memory_pool,
            performance_metrics,
            operation_queue: Arc::new(Mutex::new(Vec::new())),
            tensor_cache: Arc::new(RwLock::new(HashMap::new())),
            is_available,
        })
    }

    /// Create with default configuration
    pub fn new_default() -> Result<Self> {
        Self::new(GpuAccelerationConfig::default())
    }

    /// Check if GPU acceleration is available
    pub fn is_available(&self) -> bool {
        self.is_available
    }

    /// Get current device
    pub fn device(&self) -> &Device {
        &self.device
    }

    /// Execute tensor operation on GPU
    pub async fn execute_operation(
        &self,
        operation: TensorOperation,
    ) -> Result<TensorOperationResult> {
        let start_time = Instant::now();

        if !self.is_available && self.config.auto_fallback {
            return self.execute_on_cpu(operation).await;
        }

        // Move tensors to GPU
        let gpu_inputs = self.move_tensors_to_gpu(&operation.inputs).await?;

        // Execute operation based on type
        let outputs = match operation.operation_type {
            GpuOperationType::MatMul => {
                self.execute_matmul(&gpu_inputs, &operation.parameters)
                    .await?
            }
            GpuOperationType::Conv => {
                self.execute_conv(&gpu_inputs, &operation.parameters)
                    .await?
            }
            GpuOperationType::Embedding => {
                self.execute_embedding(&gpu_inputs, &operation.parameters)
                    .await?
            }
            GpuOperationType::Attention => {
                self.execute_attention(&gpu_inputs, &operation.parameters)
                    .await?
            }
            GpuOperationType::Activation => {
                self.execute_activation(&gpu_inputs, &operation.parameters)
                    .await?
            }
            GpuOperationType::Normalization => {
                self.execute_normalization(&gpu_inputs, &operation.parameters)
                    .await?
            }
            GpuOperationType::AudioProcessing => {
                self.execute_audio_processing(&gpu_inputs, &operation.parameters)
                    .await?
            }
            GpuOperationType::SpeakerAdaptation => {
                self.execute_speaker_adaptation(&gpu_inputs, &operation.parameters)
                    .await?
            }
        };

        let execution_time = start_time.elapsed();

        // Update performance metrics
        self.update_performance_metrics(execution_time, &operation)
            .await;

        // Estimate memory usage
        let memory_used = self.estimate_memory_usage(&gpu_inputs, &outputs);

        let mut metadata = HashMap::new();
        metadata.insert("gpu_execution".to_string(), 1.0);
        metadata.insert(
            "mixed_precision".to_string(),
            if self.config.mixed_precision {
                1.0
            } else {
                0.0
            },
        );

        Ok(TensorOperationResult {
            outputs,
            execution_time,
            memory_used,
            metadata,
        })
    }

    /// Batch execute multiple operations
    pub async fn batch_execute(
        &self,
        operations: Vec<TensorOperation>,
    ) -> Result<Vec<TensorOperationResult>> {
        if !self.is_available {
            // Fallback to sequential CPU execution
            let mut results = Vec::new();
            for op in operations {
                results.push(self.execute_on_cpu(op).await?);
            }
            return Ok(results);
        }

        // Group operations by type for batching
        let mut batched_ops: HashMap<GpuOperationType, Vec<TensorOperation>> = HashMap::new();
        for op in operations {
            batched_ops.entry(op.operation_type).or_default().push(op);
        }

        let mut results = Vec::new();

        // Execute batched operations
        for (op_type, ops) in batched_ops {
            let batch_results = self.execute_batched_operation(op_type, ops).await?;
            results.extend(batch_results);
        }

        Ok(results)
    }

    /// Move tensors to GPU memory
    async fn move_tensors_to_gpu(&self, tensors: &[Tensor]) -> Result<Vec<Tensor>> {
        let mut gpu_tensors = Vec::new();

        for tensor in tensors {
            let gpu_tensor = if !tensor.device().same_device(&self.device) {
                tensor
                    .to_device(&self.device)
                    .map_err(|e| Error::Processing(format!("Failed to move tensor to GPU: {e}")))?
            } else {
                tensor.clone()
            };
            gpu_tensors.push(gpu_tensor);
        }

        Ok(gpu_tensors)
    }

    /// Execute matrix multiplication on GPU
    async fn execute_matmul(
        &self,
        inputs: &[Tensor],
        _params: &HashMap<String, f32>,
    ) -> Result<Vec<Tensor>> {
        if inputs.len() < 2 {
            return Err(Error::Validation(
                "MatMul requires at least 2 input tensors".to_string(),
            ));
        }

        let result = inputs[0]
            .matmul(&inputs[1])
            .map_err(|e| Error::Processing(format!("GPU MatMul failed: {e}")))?;

        Ok(vec![result])
    }

    /// Execute convolution on GPU
    async fn execute_conv(
        &self,
        inputs: &[Tensor],
        params: &HashMap<String, f32>,
    ) -> Result<Vec<Tensor>> {
        if inputs.len() < 2 {
            return Err(Error::Validation(
                "Conv requires input tensor and kernel".to_string(),
            ));
        }

        let kernel_size = params.get("kernel_size").unwrap_or(&3.0) as &f32;
        let stride = params.get("stride").unwrap_or(&1.0) as &f32;

        // Simplified convolution implementation
        // In a real implementation, this would use optimized CUDA kernels
        let result = self.apply_convolution(
            &inputs[0],
            &inputs[1],
            *kernel_size as usize,
            *stride as usize,
        )?;

        Ok(vec![result])
    }

    /// Execute embedding lookup on GPU
    async fn execute_embedding(
        &self,
        inputs: &[Tensor],
        _params: &HashMap<String, f32>,
    ) -> Result<Vec<Tensor>> {
        if inputs.len() < 2 {
            return Err(Error::Validation(
                "Embedding requires indices and weights".to_string(),
            ));
        }

        // Simplified embedding lookup
        let result = inputs[1]
            .index_select(&inputs[0], 0)
            .map_err(|e| Error::Processing(format!("GPU Embedding failed: {e}")))?;

        Ok(vec![result])
    }

    /// Execute attention computation on GPU
    async fn execute_attention(
        &self,
        inputs: &[Tensor],
        params: &HashMap<String, f32>,
    ) -> Result<Vec<Tensor>> {
        if inputs.len() < 3 {
            return Err(Error::Validation(
                "Attention requires Query, Key, and Value tensors".to_string(),
            ));
        }

        let scale = params.get("scale").unwrap_or(&1.0).sqrt();

        // Simplified attention: QK^T / sqrt(d_k) * V
        let q = &inputs[0];
        let k = &inputs[1];
        let v = &inputs[2];

        let scores = q
            .matmul(&k.t()?)
            .map_err(|e| Error::Processing(format!("Attention QK computation failed: {e}")))?;

        let scale_tensor = Tensor::new(scale, &self.device)
            .map_err(|e| Error::Processing(format!("Failed to create scale tensor: {e}")))?;
        let scaled_scores = scores
            .div(&scale_tensor)
            .map_err(|e| Error::Processing(format!("Attention scaling failed: {e}")))?;

        let attention_weights = candle_nn::ops::softmax(&scaled_scores, candle_core::D::Minus1)
            .map_err(|e| Error::Processing(format!("Attention softmax failed: {e}")))?;

        let output = attention_weights
            .matmul(v)
            .map_err(|e| Error::Processing(format!("Attention output computation failed: {e}")))?;

        Ok(vec![output, attention_weights])
    }

    /// Execute activation functions on GPU
    async fn execute_activation(
        &self,
        inputs: &[Tensor],
        params: &HashMap<String, f32>,
    ) -> Result<Vec<Tensor>> {
        if inputs.is_empty() {
            return Err(Error::Validation(
                "Activation requires at least 1 input tensor".to_string(),
            ));
        }

        let activation_type = *params.get("type").unwrap_or(&0.0) as i32;
        let input = &inputs[0];

        let result = match activation_type {
            0 => input.relu()?, // ReLU
            1 => input.gelu()?, // GELU
            2 => input.tanh()?, // Tanh
            3 => {
                // Manual sigmoid implementation: 1 / (1 + exp(-x))
                let neg_input = input.neg()?;
                let exp_neg = neg_input.exp()?;
                let one = Tensor::new(1.0f32, &self.device)?;
                let denominator = one.add(&exp_neg)?;
                one.div(&denominator)?
            }
            _ => input.clone(), // Identity
        };

        Ok(vec![result])
    }

    /// Execute normalization on GPU
    async fn execute_normalization(
        &self,
        inputs: &[Tensor],
        params: &HashMap<String, f32>,
    ) -> Result<Vec<Tensor>> {
        if inputs.is_empty() {
            return Err(Error::Validation(
                "Normalization requires at least 1 input tensor".to_string(),
            ));
        }

        let eps = *params.get("eps").unwrap_or(&1e-5);
        let input = &inputs[0];

        // Layer normalization
        let mean = input.mean_keepdim(candle_core::D::Minus1)?;
        let variance =
            ((input - &mean)? * (input - &mean)?)?.mean_keepdim(candle_core::D::Minus1)?;
        let eps_tensor = Tensor::new(eps, &self.device)?;
        let std = variance.add(&eps_tensor)?.sqrt()?;
        let normalized = ((input - mean)? / std)?;

        Ok(vec![normalized])
    }

    /// Execute audio processing operations on GPU
    async fn execute_audio_processing(
        &self,
        inputs: &[Tensor],
        params: &HashMap<String, f32>,
    ) -> Result<Vec<Tensor>> {
        if inputs.is_empty() {
            return Err(Error::Validation(
                "Audio processing requires input tensor".to_string(),
            ));
        }

        let operation = *params.get("operation").unwrap_or(&0.0) as i32;
        let input = &inputs[0];

        let result = match operation {
            0 => self.apply_fft_gpu(input)?,                  // FFT
            1 => self.apply_spectrogram_gpu(input, params)?,  // Spectrogram
            2 => self.apply_mel_filter_gpu(input, params)?,   // Mel filter
            3 => self.apply_voice_filter_gpu(input, params)?, // Voice filtering
            _ => input.clone(),
        };

        Ok(vec![result])
    }

    /// Execute speaker adaptation operations on GPU
    async fn execute_speaker_adaptation(
        &self,
        inputs: &[Tensor],
        params: &HashMap<String, f32>,
    ) -> Result<Vec<Tensor>> {
        if inputs.len() < 2 {
            return Err(Error::Validation(
                "Speaker adaptation requires source and target tensors".to_string(),
            ));
        }

        let adaptation_rate = *params.get("adaptation_rate").unwrap_or(&0.1);
        let source = &inputs[0];
        let target = &inputs[1];

        // Linear interpolation adaptation
        let rate_tensor = Tensor::new(adaptation_rate, &self.device)?;
        let one_tensor = Tensor::new(1.0f32, &self.device)?;
        let inv_rate = one_tensor.sub(&rate_tensor)?;
        let adapted = source.mul(&inv_rate)?.add(&target.mul(&rate_tensor)?)?;

        Ok(vec![adapted])
    }

    /// Apply convolution (simplified)
    fn apply_convolution(
        &self,
        input: &Tensor,
        kernel: &Tensor,
        _kernel_size: usize,
        _stride: usize,
    ) -> Result<Tensor> {
        // Simplified 1D convolution for audio processing
        // In real implementation, this would use optimized CUDA convolution kernels
        input
            .conv1d(kernel, 1, 0, 1, 1)
            .map_err(|e| Error::Processing(format!("Convolution failed: {e}")))
    }

    /// Apply FFT on GPU
    fn apply_fft_gpu(&self, input: &Tensor) -> Result<Tensor> {
        // Simplified FFT - in real implementation this would use cuFFT
        // For now, return input as placeholder
        Ok(input.clone())
    }

    /// Apply spectrogram computation on GPU
    fn apply_spectrogram_gpu(
        &self,
        input: &Tensor,
        _params: &HashMap<String, f32>,
    ) -> Result<Tensor> {
        // Simplified spectrogram computation
        Ok(input.clone())
    }

    /// Apply mel filter bank on GPU
    fn apply_mel_filter_gpu(
        &self,
        input: &Tensor,
        _params: &HashMap<String, f32>,
    ) -> Result<Tensor> {
        // Simplified mel filter bank
        Ok(input.clone())
    }

    /// Apply voice filtering on GPU
    fn apply_voice_filter_gpu(
        &self,
        input: &Tensor,
        _params: &HashMap<String, f32>,
    ) -> Result<Tensor> {
        // Simplified voice filtering
        Ok(input.clone())
    }

    /// Execute operation on CPU as fallback
    async fn execute_on_cpu(&self, operation: TensorOperation) -> Result<TensorOperationResult> {
        let start_time = Instant::now();

        // Convert tensors to CPU if needed
        let cpu_device = Device::Cpu;
        let mut cpu_inputs = Vec::new();

        for tensor in &operation.inputs {
            let cpu_tensor = if !tensor.device().same_device(&cpu_device) {
                tensor
                    .to_device(&cpu_device)
                    .map_err(|e| Error::Processing(format!("Failed to move tensor to CPU: {e}")))?
            } else {
                tensor.clone()
            };
            cpu_inputs.push(cpu_tensor);
        }

        // Execute simplified CPU version
        let output = match operation.operation_type {
            GpuOperationType::MatMul => {
                if cpu_inputs.len() >= 2 {
                    vec![cpu_inputs[0]
                        .matmul(&cpu_inputs[1])
                        .map_err(|e| Error::Processing(format!("CPU MatMul failed: {e}")))?]
                } else {
                    vec![cpu_inputs[0].clone()]
                }
            }
            _ => vec![cpu_inputs[0].clone()], // Simplified fallback
        };

        let execution_time = start_time.elapsed();
        let memory_used = self.estimate_memory_usage(&cpu_inputs, &output);

        let mut metadata = HashMap::new();
        metadata.insert("cpu_fallback".to_string(), 1.0);

        Ok(TensorOperationResult {
            outputs: output,
            execution_time,
            memory_used,
            metadata,
        })
    }

    /// Execute batched operations of same type
    async fn execute_batched_operation(
        &self,
        op_type: GpuOperationType,
        operations: Vec<TensorOperation>,
    ) -> Result<Vec<TensorOperationResult>> {
        let mut results = Vec::new();

        // For simplicity, execute sequentially
        // In real implementation, this would batch operations efficiently
        for op in operations {
            let result = self.execute_operation(op).await?;
            results.push(result);
        }

        Ok(results)
    }

    /// Create GPU device
    fn create_device(config: &GpuAccelerationConfig) -> Result<Device> {
        if !config.enabled {
            return Ok(Device::Cpu);
        }

        match config.device_type {
            GpuDeviceType::Cuda => {
                #[cfg(feature = "gpu")]
                {
                    Device::new_cuda(config.device_id).map_err(|e| {
                        Error::Processing(format!("Failed to create CUDA device: {e}"))
                    })
                }
                #[cfg(not(feature = "gpu"))]
                {
                    tracing::warn!("CUDA not available, falling back to CPU");
                    Ok(Device::Cpu)
                }
            }
            GpuDeviceType::Metal => Device::new_metal(config.device_id)
                .map_err(|e| Error::Processing(format!("Failed to create Metal device: {e}"))),
            GpuDeviceType::Cpu => Ok(Device::Cpu),
        }
    }

    /// Check if GPU is available and functional
    fn check_gpu_availability(device: &Device) -> bool {
        match device {
            Device::Cpu => false,
            _ => {
                // Try to create a simple tensor to test GPU functionality
                match Tensor::zeros((2, 2), DType::F32, device) {
                    Ok(_) => true,
                    Err(_) => false,
                }
            }
        }
    }

    /// Update performance metrics
    async fn update_performance_metrics(
        &self,
        execution_time: Duration,
        operation: &TensorOperation,
    ) {
        let mut metrics = self.performance_metrics.write().unwrap();

        metrics.operations_count += 1;

        // Update average kernel time
        let ops_count = metrics.operations_count;
        let total_time = metrics.avg_kernel_time * (ops_count as u32 - 1) + execution_time;
        metrics.avg_kernel_time = total_time / (ops_count as u32);

        // Simulate GPU utilization (in real implementation, would query GPU)
        metrics.gpu_utilization = 0.75 + fastrand::f32() * 0.2; // 75-95%
        metrics.memory_bandwidth = 0.6 + fastrand::f32() * 0.3; // 60-90%
        metrics.temperature = 45.0 + fastrand::f32() * 20.0; // 45-65Â°C
        metrics.power_consumption = 150.0 + fastrand::f32() * 100.0; // 150-250W
    }

    /// Estimate memory usage for operations
    fn estimate_memory_usage(&self, inputs: &[Tensor], outputs: &[Tensor]) -> u64 {
        let input_memory: u64 = inputs
            .iter()
            .map(|t| t.elem_count() * t.dtype().size_in_bytes())
            .sum::<usize>() as u64;

        let output_memory: u64 = outputs
            .iter()
            .map(|t| t.elem_count() * t.dtype().size_in_bytes())
            .sum::<usize>() as u64;

        input_memory + output_memory
    }

    /// Get current memory statistics
    pub fn get_memory_stats(&self) -> GpuMemoryStats {
        self.memory_pool.lock().unwrap().get_stats()
    }

    /// Get current performance metrics
    pub fn get_performance_metrics(&self) -> GpuPerformanceMetrics {
        self.performance_metrics.read().unwrap().clone()
    }

    /// Get cached tensor if available
    pub fn get_cached_tensor(&self, key: &str) -> Option<Tensor> {
        self.tensor_cache.read().unwrap().get(key).cloned()
    }

    /// Cache tensor for reuse
    pub fn cache_tensor(&self, key: String, tensor: Tensor) {
        self.tensor_cache.write().unwrap().insert(key, tensor);
    }

    /// Clear tensor cache
    pub fn clear_cache(&self) {
        self.tensor_cache.write().unwrap().clear();
    }

    /// Synchronize GPU operations
    pub async fn synchronize(&self) -> Result<()> {
        // In real implementation, this would call cudaDeviceSynchronize or equivalent
        tokio::time::sleep(Duration::from_millis(1)).await;
        Ok(())
    }

    /// Warm up GPU (pre-allocate memory pools, compile kernels)
    pub async fn warmup(&self) -> Result<()> {
        if !self.is_available {
            return Ok(());
        }

        tracing::info!("Warming up GPU acceleration...");

        // Create some test tensors to warm up the GPU
        let test_sizes = vec![(32, 32), (64, 64), (128, 128), (256, 256)];

        for (rows, cols) in test_sizes {
            let a = Tensor::randn(0.0, 1.0, (rows, cols), &self.device)
                .map_err(|e| Error::Processing(format!("Failed to create warmup tensor: {e}")))?;
            let b = Tensor::randn(0.0, 1.0, (cols, rows), &self.device)
                .map_err(|e| Error::Processing(format!("Failed to create warmup tensor: {e}")))?;

            // Perform warmup operations
            let _result = a
                .matmul(&b)
                .map_err(|e| Error::Processing(format!("Warmup operation failed: {e}")))?;
        }

        self.synchronize().await?;
        tracing::info!("GPU acceleration warmup completed");

        Ok(())
    }

    /// Get device information
    pub fn get_device_info(&self) -> HashMap<String, String> {
        let mut info = HashMap::new();

        match &self.device {
            Device::Cpu => {
                info.insert("type".to_string(), "CPU".to_string());
                info.insert("name".to_string(), "CPU".to_string());
            }
            Device::Cuda(device) => {
                info.insert("type".to_string(), "CUDA".to_string());
                info.insert("device_id".to_string(), "0".to_string()); // Simplified
                info.insert("name".to_string(), "NVIDIA GPU".to_string());
            }
            Device::Metal(device) => {
                info.insert("type".to_string(), "Metal".to_string());
                info.insert("device_id".to_string(), "0".to_string()); // Simplified
                info.insert("name".to_string(), "Apple GPU".to_string());
            }
        }

        info.insert("available".to_string(), self.is_available.to_string());
        info.insert(
            "mixed_precision".to_string(),
            self.config.mixed_precision.to_string(),
        );
        info.insert(
            "tensor_cores".to_string(),
            self.config.use_tensor_cores.to_string(),
        );

        info
    }
}

/// GPU acceleration utilities
pub struct GpuUtils;

impl GpuUtils {
    /// Check if CUDA is available on the system
    pub fn is_cuda_available() -> bool {
        #[cfg(feature = "gpu")]
        {
            Device::new_cuda(0).is_ok()
        }
        #[cfg(not(feature = "gpu"))]
        {
            false
        }
    }

    /// Check if Metal is available on the system
    pub fn is_metal_available() -> bool {
        Device::new_metal(0).is_ok()
    }

    /// Get list of available GPU devices
    pub fn list_gpu_devices() -> Vec<HashMap<String, String>> {
        let mut devices = Vec::new();

        // Check CUDA devices
        #[cfg(feature = "gpu")]
        {
            for i in 0..8 {
                if let Ok(device) = Device::new_cuda(i) {
                    let mut info = HashMap::new();
                    info.insert("type".to_string(), "CUDA".to_string());
                    info.insert("id".to_string(), i.to_string());
                    info.insert("available".to_string(), "true".to_string());
                    devices.push(info);
                } else {
                    break;
                }
            }
        }

        // Check Metal devices
        for i in 0..4 {
            if let Ok(_device) = Device::new_metal(i) {
                let mut info = HashMap::new();
                info.insert("type".to_string(), "Metal".to_string());
                info.insert("id".to_string(), i.to_string());
                info.insert("available".to_string(), "true".to_string());
                devices.push(info);
            } else {
                break;
            }
        }

        devices
    }

    /// Get optimal device configuration for current system
    pub fn get_optimal_config() -> GpuAccelerationConfig {
        let mut config = GpuAccelerationConfig::default();

        if Self::is_cuda_available() {
            config.device_type = GpuDeviceType::Cuda;
        } else if Self::is_metal_available() {
            config.device_type = GpuDeviceType::Metal;
        } else {
            config.device_type = GpuDeviceType::Cpu;
            config.enabled = false;
        }

        config
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_gpu_config_creation() {
        let config = GpuAccelerationConfig::default();
        assert!(config.enabled);
        assert_eq!(config.device_type, GpuDeviceType::Cuda);
        assert!(config.mixed_precision);
    }

    #[test]
    fn test_gpu_utils() {
        let devices = GpuUtils::list_gpu_devices();
        // Should not panic
        assert!(devices.len() >= 0);

        let config = GpuUtils::get_optimal_config();
        assert!(
            config.device_type == GpuDeviceType::Cuda
                || config.device_type == GpuDeviceType::Metal
                || config.device_type == GpuDeviceType::Cpu
        );
    }

    #[tokio::test]
    async fn test_gpu_accelerator_creation() {
        let config = GpuAccelerationConfig {
            enabled: true,
            auto_fallback: true,
            ..Default::default()
        };

        let accelerator = GpuAccelerator::new(config);
        // Should not panic even if GPU is not available due to auto_fallback
        assert!(accelerator.is_ok());
    }

    #[tokio::test]
    async fn test_tensor_operation() {
        let config = GpuAccelerationConfig {
            enabled: false, // Use CPU for testing
            auto_fallback: true,
            ..Default::default()
        };

        let accelerator = GpuAccelerator::new(config).unwrap();

        // Create test tensors
        let a = Tensor::zeros((2, 3), DType::F32, accelerator.device()).unwrap();
        let b = Tensor::zeros((3, 2), DType::F32, accelerator.device()).unwrap();

        let operation = TensorOperation {
            operation_type: GpuOperationType::MatMul,
            inputs: vec![a, b],
            parameters: HashMap::new(),
            priority: 5,
        };

        let result = accelerator.execute_operation(operation).await;
        assert!(result.is_ok());

        let op_result = result.unwrap();
        assert_eq!(op_result.outputs.len(), 1);
        assert!(op_result.execution_time > Duration::from_nanos(0));
    }

    #[tokio::test]
    async fn test_batch_execution() {
        let config = GpuAccelerationConfig {
            enabled: false, // Use CPU for testing
            auto_fallback: true,
            batch_size: 4,
            ..Default::default()
        };

        let accelerator = GpuAccelerator::new(config).unwrap();

        let mut operations = Vec::new();
        for _ in 0..3 {
            let a = Tensor::ones((2, 2), DType::F32, accelerator.device()).unwrap();
            let operation = TensorOperation {
                operation_type: GpuOperationType::Activation,
                inputs: vec![a],
                parameters: {
                    let mut params = HashMap::new();
                    params.insert("type".to_string(), 0.0); // ReLU
                    params
                },
                priority: 3,
            };
            operations.push(operation);
        }

        let results = accelerator.batch_execute(operations).await;
        assert!(results.is_ok());

        let batch_results = results.unwrap();
        assert_eq!(batch_results.len(), 3);
    }

    #[test]
    fn test_memory_pool() {
        let device = Device::Cpu;
        let mut pool = GpuMemoryPool::new(device);

        let ptr_id = pool.allocate(1024).unwrap();
        assert_eq!(ptr_id, 1024); // Returns size as mock pointer ID

        let stats = pool.get_stats();
        assert!(stats.total_memory > 0);
    }

    #[tokio::test]
    async fn test_gpu_warmup() {
        let config = GpuAccelerationConfig {
            enabled: false, // Use CPU for testing
            ..Default::default()
        };

        let accelerator = GpuAccelerator::new(config).unwrap();
        let result = accelerator.warmup().await;
        assert!(result.is_ok());
    }

    #[test]
    fn test_device_info() {
        let config = GpuAccelerationConfig {
            enabled: false,
            ..Default::default()
        };

        let accelerator = GpuAccelerator::new(config).unwrap();
        let info = accelerator.get_device_info();

        assert!(info.contains_key("type"));
        assert!(info.contains_key("available"));
        assert_eq!(info.get("type").unwrap(), "CPU");
    }
}
